#!/bin/bash
set -e # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# ==========================================
# ç”¨æˆ·é…ç½®åŒº
# ==========================================
# [å¿…å¡«] è¯·å°†ä¸‹é¢çš„ ID æ›¿æ¢ä¸ºä½ çš„ Google Drive æ–‡ä»¶ ID (åˆ†äº«é“¾æ¥ä¸­ 'd/' å’Œ '/view' ä¹‹é—´çš„éƒ¨åˆ†)
GDRIVE_ID="1-KNTtbE_01KBzFiueswuTxvTTNERUsaG"

# [å¯é€‰] æ•°æ®é›†æ–‡ä»¶åå’Œè§£å‹ç›®å½•
DATASET_ARCHIVE="completefile.zip"
DATASET_DIR="completefile"

# [å¯é€‰] æ¨¡å‹å­˜æ”¾ç›®å½•
MODEL_DIR="models"
GEMMA_DIR="$MODEL_DIR/gemma"

# [å¯é€‰] Hugging Face Repo ID
LTX_MODEL_REPO="Lightricks/LTX-2"
LTX_MODEL_FILENAME="ltx-2-19b-dev.safetensors"
TEXT_ENCODER_REPO="google/gemma-3-12b-it-qat-q4_0-unquantized"

# ==========================================

echo "ğŸš€ å¼€å§‹ä¸€é”®è®­ç»ƒæµç¨‹..."

# 0. æ£€æŸ¥è¿è¡Œç›®å½• (é’ˆå¯¹ Vast.ai/RunPod ä¼˜åŒ–)
if [ -d "/workspace" ] && [[ "$PWD" != "/workspace"* ]]; then
    echo "âš ï¸  ã€è­¦å‘Šã€‘æ£€æµ‹åˆ° /workspace ç›®å½•ï¼Œä½†å½“å‰è„šæœ¬è¿è¡Œåœ¨ $PWD ä¸‹ã€‚"
    echo "     /workspace é€šå¸¸æ˜¯æŒä¹…åŒ–å¤§å®¹é‡å­˜å‚¨ï¼Œè€Œ $PWD å¯èƒ½æ˜¯ Docker ä¸´æ—¶å±‚ (ç©ºé—´æœ‰é™)ã€‚"
    echo "     å¼ºçƒˆå»ºè®®åœæ­¢å½“å‰è„šæœ¬ï¼Œå°† LTX-2 æ–‡ä»¶å¤¹ç§»åŠ¨åˆ° /workspace åå†è¿è¡Œã€‚"
    echo "     (ä¾‹å¦‚: mv ~/LTX-2 /workspace/ && cd /workspace/LTX-2)"
    echo "     æ­£åœ¨æš‚åœ 10 ç§’ï¼ŒæŒ‰ Ctrl+C å¯ä¸­æ­¢..."
    sleep 10
fi

# æ£€æŸ¥å½“å‰ç›®å½•
if [ ! -f "scripts/process_dataset.py" ]; then
    echo "âŒ é”™è¯¯ï¼šè¯·åœ¨ 'packages/ltx-trainer' ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬ã€‚"
    exit 1
fi

# 1. æ£€æŸ¥å¹¶å®‰è£…å¿…è¦å·¥å…·
echo "ğŸ› ï¸  æ£€æŸ¥ç¯å¢ƒ..."

# å®‰è£… uv (å¦‚æœæœªå®‰è£…)
if ! command -v uv &> /dev/null; then
    echo "æ­£åœ¨å®‰è£… uv..."
    curl -LsSf https://astral.sh/uv/install.sh | sh
    source $HOME/.local/bin/env
    echo "uv å·²å®‰è£…ã€‚"
fi

# æ£€æŸ¥å¹¶å®‰è£… unzip
if ! command -v unzip &> /dev/null; then
    echo "æ­£åœ¨å®‰è£… unzip..."
    if command -v apt-get &> /dev/null; then
        apt-get update && apt-get install -y unzip
    elif command -v yum &> /dev/null; then
        yum install -y unzip
    else
        echo "âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° unzipï¼Œä¸”æ— æ³•è‡ªåŠ¨å®‰è£…ã€‚è¯·æ‰‹åŠ¨å®‰è£…ã€‚"
        exit 1
    fi
else
    echo "unzip å·²å®‰è£…ã€‚"
fi

# å®‰è£… python ä¾èµ–å·¥å…·
echo "å®‰è£…å·¥å…·ä¾èµ– (gdown, huggingface_hub)..."
pip install gdown huggingface_hub --upgrade --quiet

# 2. ä¸‹è½½æ•°æ®é›†
echo "ğŸ“¥ å‡†å¤‡æ•°æ®é›†..."
if [ ! -d "$DATASET_DIR" ]; then
    if [ "$GDRIVE_ID" == "<YOUR_GDRIVE_FILE_ID>" ]; then
        echo "âŒ é”™è¯¯ï¼šè¯·å…ˆåœ¨è„šæœ¬ä¸­é…ç½® GDRIVE_IDï¼"
        exit 1
    fi
    
    echo "ä» Google Drive ä¸‹è½½æ•°æ®é›† (ID: $GDRIVE_ID)..."
    gdown "$GDRIVE_ID" -O "$DATASET_ARCHIVE"
    
    echo "ğŸ“¦ è§£å‹æ•°æ®é›†..."
    unzip -o "$DATASET_ARCHIVE" -d "$DATASET_DIR"
else
    echo "æ•°æ®é›†ç›®å½• '$DATASET_DIR' å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚"
fi

# 3. ä¸‹è½½æ¨¡å‹
echo "ğŸ“¥ å‡†å¤‡æ¨¡å‹..."
mkdir -p "$MODEL_DIR"

# ä¸‹è½½ LTX-2
LTX_MODEL_PATH="$MODEL_DIR/$LTX_MODEL_FILENAME"
if [ ! -f "$LTX_MODEL_PATH" ]; then
    echo "ä¸‹è½½ LTX-2 æ¨¡å‹ ($LTX_MODEL_REPO)..."
    python3 -c "from huggingface_hub import hf_hub_download; hf_hub_download(repo_id='$LTX_MODEL_REPO', filename='$LTX_MODEL_FILENAME', local_dir='$MODEL_DIR', local_dir_use_symlinks=False)"
else
    echo "LTX-2 æ¨¡å‹å·²å­˜åœ¨ã€‚"
fi

# ä¸‹è½½ Gemma
if [ ! -d "$GEMMA_DIR" ]; then
    echo "ä¸‹è½½ Gemma æ–‡æœ¬ç¼–ç å™¨ ($TEXT_ENCODER_REPO)..."
    
    # æ£€æŸ¥æ˜¯å¦å·²ç™»å½• Hugging Face (Gemma æ¨¡å‹éœ€è¦æƒé™)
    if ! python3 -c "from huggingface_hub import HfFolder; exit(0 if HfFolder.get_token() else 1)"; then
        echo "âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° Hugging Face ç™»å½•çŠ¶æ€ï¼"
        echo "Gemma æ¨¡å‹å±äºå—é™èµ„æºï¼Œè¯·å…ˆè¿è¡Œ 'huggingface-cli login' å¹¶è¾“å…¥æ‚¨çš„ Access Tokenã€‚"
        echo "Token è·å–åœ°å€: https://huggingface.co/settings/tokens"
        exit 1
    fi

    python3 -c "from huggingface_hub import snapshot_download; snapshot_download(repo_id='$TEXT_ENCODER_REPO', local_dir='$GEMMA_DIR', local_dir_use_symlinks=False)"
else
    echo "Gemma æ¨¡å‹å·²å­˜åœ¨ã€‚"
fi

# 4. é¢„å¤„ç†
echo "âš™ï¸  å¼€å§‹é¢„å¤„ç†..."

# è‡ªåŠ¨æŸ¥æ‰¾ dataset.json
DATASET_JSON=$(find "$DATASET_DIR" -maxdepth 2 -name "*.json" | head -n 1)

if [ -z "$DATASET_JSON" ]; then
    echo "âŒ é”™è¯¯ï¼šåœ¨ $DATASET_DIR ä¸­æœªæ‰¾åˆ° .json æ•°æ®é›†æ–‡ä»¶ã€‚"
    echo "è¯·ç¡®ä¿è§£å‹åçš„ç›®å½•ä¸­åŒ…å« dataset.json æ–‡ä»¶ã€‚"
    exit 1
fi

echo "ä½¿ç”¨æ•°æ®é›†æ–‡ä»¶: $DATASET_JSON"

# è¿è¡Œé¢„å¤„ç†
# æ³¨æ„ï¼šåˆ†è¾¨ç‡ buckets å¯ä»¥æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´
uv run scripts/process_dataset.py "$DATASET_JSON" \
    --resolution-buckets "960x544x49" \
    --model-path "$LTX_MODEL_PATH" \
    --text-encoder-path "$GEMMA_DIR"

# 5. è®­ç»ƒ
echo "ğŸ”¥ å¼€å§‹è®­ç»ƒ..."
# é»˜è®¤ä½¿ç”¨ LoRA é…ç½®ï¼Œå¦‚æœéœ€è¦å…¨é‡å¾®è°ƒè¯·ä¿®æ”¹æ­¤å¤„çš„é…ç½®æ–‡ä»¶è·¯å¾„
uv run scripts/train.py configs/ltx2_av_lora.yaml

echo "âœ… è®­ç»ƒæµç¨‹å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ä½äº runs/ ç›®å½•ã€‚"
