#!/bin/bash
set -e # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# ==========================================
# ç”¨æˆ·é…ç½®åŒº
# ==========================================
# [å¿…å¡«] è¯·å°†ä¸‹é¢çš„ ID æ›¿æ¢ä¸ºä½ çš„ Google Drive æ–‡ä»¶ ID (åˆ†äº«é“¾æ¥ä¸­ 'd/' å’Œ '/view' ä¹‹é—´çš„éƒ¨åˆ†)
GDRIVE_ID="1Ke3BTygkL4IOJXnG8GXs3ITK0nFp5WRz"

# [å¯é€‰] æ•°æ®é›†æ–‡ä»¶åå’Œè§£å‹ç›®å½•
DATASET_ARCHIVE="completefile.zip"
DATASET_DIR="completefile"

# [å¯é€‰] æ¨¡å‹å­˜æ”¾ç›®å½•
MODEL_DIR="models"

# [å¯é€‰] Hugging Face Repo ID
LTX_MODEL_REPO="Lightricks/LTX-2"
LTX_MODEL_FILENAME="ltx-2-19b-dev.safetensors"
TEXT_ENCODER_REPO="google/gemma-3-12b-it-qat-q4_0-unquantized"

# ==========================================

echo "ğŸš€ å¼€å§‹ä¸€é”®è®­ç»ƒæµç¨‹..."

# 0. æ£€æŸ¥å½“å‰ç›®å½•
if [ ! -f "scripts/process_dataset.py" ]; then
    echo "âŒ é”™è¯¯ï¼šè¯·åœ¨ 'packages/ltx-trainer' ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬ã€‚"
    exit 1
fi

# 1. æ£€æŸ¥å¹¶å®‰è£…å¿…è¦å·¥å…·
echo "ğŸ› ï¸  æ£€æŸ¥ç¯å¢ƒ..."

# å®‰è£… uv (å¦‚æœæœªå®‰è£…)
if ! command -v uv &> /dev/null; then
    echo "æ­£åœ¨å®‰è£… uv..."
    curl -LsSf https://astral.sh/uv/install.sh | sh
    source $HOME/.cargo/env
else
    echo "uv å·²å®‰è£…ã€‚"
fi

# å®‰è£… python ä¾èµ–å·¥å…·
echo "å®‰è£…å·¥å…·ä¾èµ– (gdown, huggingface_hub)..."
pip install gdown huggingface_hub --upgrade --quiet

# 2. ä¸‹è½½æ•°æ®é›†
echo "ğŸ“¥ å‡†å¤‡æ•°æ®é›†..."
if [ ! -d "$DATASET_DIR" ]; then
    if [ "$GDRIVE_ID" == "<YOUR_GDRIVE_FILE_ID>" ]; then
        echo "âŒ é”™è¯¯ï¼šè¯·å…ˆåœ¨è„šæœ¬ä¸­é…ç½® GDRIVE_IDï¼"
        exit 1
    fi
    
    echo "ä» Google Drive ä¸‹è½½æ•°æ®é›† (ID: $GDRIVE_ID)..."
    gdown "$GDRIVE_ID" -O "$DATASET_ARCHIVE"
    
    echo "ğŸ“¦ è§£å‹æ•°æ®é›†..."
    unzip -o "$DATASET_ARCHIVE" -d "$DATASET_DIR"
else
    echo "æ•°æ®é›†ç›®å½• '$DATASET_DIR' å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚"
fi

# 3. ä¸‹è½½æ¨¡å‹
echo "ğŸ“¥ å‡†å¤‡æ¨¡å‹..."
mkdir -p "$MODEL_DIR"

# ä¸‹è½½ LTX-2
LTX_MODEL_PATH="$MODEL_DIR/$LTX_MODEL_FILENAME"
if [ ! -f "$LTX_MODEL_PATH" ]; then
    echo "ä¸‹è½½ LTX-2 æ¨¡å‹ ($LTX_MODEL_REPO)..."
    huggingface-cli download "$LTX_MODEL_REPO" "$LTX_MODEL_FILENAME" --local-dir "$MODEL_DIR" --local-dir-use-symlinks False
else
    echo "LTX-2 æ¨¡å‹å·²å­˜åœ¨ã€‚"
fi

# ä¸‹è½½ Gemma
if [ ! -d "$GEMMA_DIR" ]; then
    echo "ä¸‹è½½ Gemma æ–‡æœ¬ç¼–ç å™¨ ($TEXT_ENCODER_REPO)..."
    
    # æ£€æŸ¥æ˜¯å¦å·²ç™»å½• Hugging Face (Gemma æ¨¡å‹éœ€è¦æƒé™)
    if ! huggingface-cli whoami &> /dev/null; then
        echo "âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° Hugging Face ç™»å½•çŠ¶æ€ï¼"
        echo "Gemma æ¨¡å‹å±äºå—é™èµ„æºï¼Œè¯·å…ˆè¿è¡Œ 'huggingface-cli login' å¹¶è¾“å…¥æ‚¨çš„ Access Tokenã€‚"
        echo "Token è·å–åœ°å€: https://huggingface.co/settings/tokens"
        exit 1
    fi

    huggingface-cli download "$TEXT_ENCODER_REPO" --local-dir "$GEMMA_DIR" --local-dir-use-symlinks False
else
    echo "Gemma æ¨¡å‹å·²å­˜åœ¨ã€‚"
fi

# 4. é¢„å¤„ç†
echo "âš™ï¸  å¼€å§‹é¢„å¤„ç†..."

# è‡ªåŠ¨æŸ¥æ‰¾ dataset.json
DATASET_JSON=$(find "$DATASET_DIR" -maxdepth 2 -name "*.json" | head -n 1)

if [ -z "$DATASET_JSON" ]; then
    echo "âŒ é”™è¯¯ï¼šåœ¨ $DATASET_DIR ä¸­æœªæ‰¾åˆ° .json æ•°æ®é›†æ–‡ä»¶ã€‚"
    echo "è¯·ç¡®ä¿è§£å‹åçš„ç›®å½•ä¸­åŒ…å« dataset.json æ–‡ä»¶ã€‚"
    exit 1
fi

echo "ä½¿ç”¨æ•°æ®é›†æ–‡ä»¶: $DATASET_JSON"

# è¿è¡Œé¢„å¤„ç†
# æ³¨æ„ï¼šåˆ†è¾¨ç‡ buckets å¯ä»¥æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´
uv run scripts/process_dataset.py "$DATASET_JSON" \
    --resolution-buckets "960x544x49" \
    --model-path "$LTX_MODEL_PATH" \
    --text-encoder-path "$GEMMA_DIR"

# 5. è®­ç»ƒ
echo "ğŸ”¥ å¼€å§‹è®­ç»ƒ..."
# é»˜è®¤ä½¿ç”¨ LoRA é…ç½®ï¼Œå¦‚æœéœ€è¦å…¨é‡å¾®è°ƒè¯·ä¿®æ”¹æ­¤å¤„çš„é…ç½®æ–‡ä»¶è·¯å¾„
uv run scripts/train.py configs/ltx2_av_lora.yaml

echo "âœ… è®­ç»ƒæµç¨‹å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ä½äº runs/ ç›®å½•ã€‚"
