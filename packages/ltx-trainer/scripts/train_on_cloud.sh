#!/bin/bash
set -e # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# ==========================================
# ç”¨æˆ·é…ç½®åŒº
# ==========================================
# [å¿…å¡«] è¯·å°†ä¸‹é¢çš„ ID æ›¿æ¢ä¸ºä½ çš„ Google Drive æ–‡ä»¶ ID (åˆ†äº«é“¾æ¥ä¸­ 'd/' å’Œ '/view' ä¹‹é—´çš„éƒ¨åˆ†)
GDRIVE_ID="1-KNTtbE_01KBzFiueswuTxvTTNERUsaG"

# [å¯é€‰] æ•°æ®é›†æ–‡ä»¶åå’Œè§£å‹ç›®å½•
DATASET_ARCHIVE="completefile.zip"
DATASET_DIR="completefile"

# [å¯é€‰] æ¨¡å‹å­˜æ”¾ç›®å½•
MODEL_DIR="models"
GEMMA_DIR="$MODEL_DIR/gemma"

# [å¯é€‰] Hugging Face Repo ID
LTX_MODEL_REPO="Lightricks/LTX-2"
LTX_MODEL_FILENAME="ltx-2-19b-dev.safetensors"
TEXT_ENCODER_REPO="google/gemma-3-12b-it-qat-q4_0-unquantized"

# ==========================================

echo "ğŸš€ å¼€å§‹ä¸€é”®è®­ç»ƒæµç¨‹..."

# 0. æ£€æŸ¥è¿è¡Œç›®å½• (é’ˆå¯¹ Vast.ai/RunPod ä¼˜åŒ–)
if [ -d "/workspace" ] && [[ "$PWD" != "/workspace"* ]]; then
    echo "âš ï¸  ã€è­¦å‘Šã€‘æ£€æµ‹åˆ° /workspace ç›®å½•ï¼Œä½†å½“å‰è„šæœ¬è¿è¡Œåœ¨ $PWD ä¸‹ã€‚"
    echo "     /workspace é€šå¸¸æ˜¯æŒä¹…åŒ–å¤§å®¹é‡å­˜å‚¨ï¼Œè€Œ $PWD å¯èƒ½æ˜¯ Docker ä¸´æ—¶å±‚ (ç©ºé—´æœ‰é™)ã€‚"
    echo "     å¼ºçƒˆå»ºè®®åœæ­¢å½“å‰è„šæœ¬ï¼Œå°† LTX-2 æ–‡ä»¶å¤¹ç§»åŠ¨åˆ° /workspace åå†è¿è¡Œã€‚"
    echo "     (ä¾‹å¦‚: mv ~/LTX-2 /workspace/ && cd /workspace/LTX-2)"
    echo "     æ­£åœ¨æš‚åœ 10 ç§’ï¼ŒæŒ‰ Ctrl+C å¯ä¸­æ­¢..."
    sleep 10
fi

# æ£€æŸ¥å½“å‰ç›®å½•
if [ ! -f "scripts/process_dataset.py" ]; then
    echo "âŒ é”™è¯¯ï¼šè¯·åœ¨ 'packages/ltx-trainer' ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬ã€‚"
    exit 1
fi

# 1. æ£€æŸ¥å¹¶å®‰è£…å¿…è¦å·¥å…·
echo "ğŸ› ï¸  æ£€æŸ¥ç¯å¢ƒ..."

# å®‰è£… uv (å¦‚æœæœªå®‰è£…)
if ! command -v uv &> /dev/null; then
    echo "æ­£åœ¨å®‰è£… uv..."
    curl -LsSf https://astral.sh/uv/install.sh | sh
    source $HOME/.local/bin/env
    echo "uv å·²å®‰è£…ã€‚"
fi

# æ£€æŸ¥å¹¶å®‰è£… unzip
if ! command -v unzip &> /dev/null; then
    echo "æ­£åœ¨å®‰è£… unzip..."
    if command -v apt-get &> /dev/null; then
        apt-get update && apt-get install -y unzip
    elif command -v yum &> /dev/null; then
        yum install -y unzip
    else
        echo "âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° unzipï¼Œä¸”æ— æ³•è‡ªåŠ¨å®‰è£…ã€‚è¯·æ‰‹åŠ¨å®‰è£…ã€‚"
        exit 1
    fi
else
    echo "unzip å·²å®‰è£…ã€‚"
fi

# å®‰è£… python ä¾èµ–å·¥å…·
echo "å®‰è£…å·¥å…·ä¾èµ– (gdown, huggingface_hub)..."
# å¼ºåˆ¶é‡æ–°å®‰è£… huggingface_hub ä»¥è§£å†³ç‰ˆæœ¬å†²çª (å¦‚ 1.3.2 é—®é¢˜)
pip install gdown huggingface_hub --upgrade --force-reinstall --quiet

# 2. ä¸‹è½½æ•°æ®é›†
echo "ğŸ“¥ å‡†å¤‡æ•°æ®é›†..."
if [ ! -d "$DATASET_DIR" ]; then
    if [ "$GDRIVE_ID" == "<YOUR_GDRIVE_FILE_ID>" ]; then
        echo "âŒ é”™è¯¯ï¼šè¯·å…ˆåœ¨è„šæœ¬ä¸­é…ç½® GDRIVE_IDï¼"
        exit 1
    fi
    
    echo "ä» Google Drive ä¸‹è½½æ•°æ®é›† (ID: $GDRIVE_ID)..."
    gdown "$GDRIVE_ID" -O "$DATASET_ARCHIVE"
    
    echo "ğŸ“¦ è§£å‹æ•°æ®é›†..."
    unzip -o "$DATASET_ARCHIVE" -d "$DATASET_DIR"
else
    echo "æ•°æ®é›†ç›®å½• '$DATASET_DIR' å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚"
fi

# 3. ä¸‹è½½æ¨¡å‹
echo "ğŸ“¥ å‡†å¤‡æ¨¡å‹..."
mkdir -p "$MODEL_DIR"

# ä¸‹è½½ LTX-2
LTX_MODEL_PATH="$MODEL_DIR/$LTX_MODEL_FILENAME"
if [ ! -f "$LTX_MODEL_PATH" ]; then
    echo "ä¸‹è½½ LTX-2 æ¨¡å‹ ($LTX_MODEL_REPO)..."
    python3 -c "from huggingface_hub import hf_hub_download; hf_hub_download(repo_id='$LTX_MODEL_REPO', filename='$LTX_MODEL_FILENAME', local_dir='$MODEL_DIR', local_dir_use_symlinks=False)"
else
    echo "LTX-2 æ¨¡å‹å·²å­˜åœ¨ã€‚"
fi

# ä¸‹è½½ Gemma
# æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ¨¡å‹æ–‡ä»¶ (é˜²æ­¢ä¸‹è½½ä¸­æ–­å¯¼è‡´çš„ç©ºç›®å½•)
if ! ls "$GEMMA_DIR"/model*.safetensors >/dev/null 2>&1; then
    echo "ä¸‹è½½ Gemma æ–‡æœ¬ç¼–ç å™¨ ($TEXT_ENCODER_REPO)..."
    
    # æ¸…ç†å¯èƒ½æ®‹ç•™çš„ç©ºç›®å½•
    if [ -d "$GEMMA_DIR" ]; then
        echo "å‘ç°ä¸å®Œæ•´çš„ Gemma ç›®å½•ï¼Œæ­£åœ¨æ¸…ç†..."
        rm -rf "$GEMMA_DIR"
    fi
    
    # æ£€æŸ¥æ˜¯å¦å·²ç™»å½• Hugging Face (Gemma æ¨¡å‹éœ€è¦æƒé™)
    if ! python3 -c "import huggingface_hub; exit(0 if huggingface_hub.get_token() else 1)"; then
        echo "âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° Hugging Face ç™»å½•çŠ¶æ€ï¼"
        echo "Gemma æ¨¡å‹å±äºå—é™èµ„æºï¼Œè¯·è¾“å…¥æ‚¨çš„ Access Token è¿›è¡Œç™»å½•ã€‚"
        echo "Token è·å–åœ°å€: https://huggingface.co/settings/tokens"
        echo ""
        echo "ğŸ” è¯·å¤åˆ¶å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œç™»å½•:"
        echo "python3 -c \"import huggingface_hub; huggingface_hub.login()\""
        exit 1
    fi

    # ä½¿ç”¨ python è„šæœ¬ä¸‹è½½å¹¶å¤„ç†å¼‚å¸¸
    python3 -c "
from huggingface_hub import snapshot_download
from huggingface_hub.utils import HfHubHTTPError
import sys

try:
    snapshot_download(repo_id='$TEXT_ENCODER_REPO', local_dir='$GEMMA_DIR', local_dir_use_symlinks=False)
except HfHubHTTPError as e:
    print(f'\nâŒ ä¸‹è½½å¤±è´¥: {e}')
    if '403' in str(e):
        print('\nğŸ›‘ æƒé™è¢«æ‹’ç» (403 Forbidden) è§£å†³æ–¹æ¡ˆ:')
        print('1. è¯·ç¡®ä¿æ‚¨å·²åœ¨ Hugging Face å®˜ç½‘åŒæ„ Gemma-3 çš„ä½¿ç”¨åè®®: https://huggingface.co/google/gemma-3-12b-it')
        print('2. è¯·æ£€æŸ¥æ‚¨çš„ Access Token æƒé™ (Fine-grained tokens éœ€è¦å¼€å¯ \'Gated repositories\' è¯»å–æƒé™)ã€‚')
        print('3. å°è¯•é‡æ–°ç”Ÿæˆä¸€ä¸ª Token å¹¶é€šè¿‡ python3 -c \"import huggingface_hub; huggingface_hub.login()\" é‡æ–°ç™»å½•ã€‚')
    sys.exit(1)
except Exception as e:
    print(f'\nâŒ æœªçŸ¥é”™è¯¯: {e}')
    sys.exit(1)
"
else
    echo "Gemma æ¨¡å‹å·²å­˜åœ¨ã€‚"
fi

# 4. é¢„å¤„ç†
echo "âš™ï¸  å¼€å§‹é¢„å¤„ç†..."

# è‡ªåŠ¨æŸ¥æ‰¾ dataset.json
DATASET_JSON=$(find "$DATASET_DIR" -maxdepth 2 -name "*.json" | head -n 1)

if [ -z "$DATASET_JSON" ]; then
    echo "âŒ é”™è¯¯ï¼šåœ¨ $DATASET_DIR ä¸­æœªæ‰¾åˆ° .json æ•°æ®é›†æ–‡ä»¶ã€‚"
    echo "è¯·ç¡®ä¿è§£å‹åçš„ç›®å½•ä¸­åŒ…å« dataset.json æ–‡ä»¶ã€‚"
    exit 1
fi

echo "ä½¿ç”¨æ•°æ®é›†æ–‡ä»¶: $DATASET_JSON"

# è¿è¡Œé¢„å¤„ç†
# æ³¨æ„ï¼šåˆ†è¾¨ç‡ buckets å¯ä»¥æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´
uv run scripts/process_dataset.py "$DATASET_JSON" \
    --resolution-buckets "960x544x49" \
    --model-path "$LTX_MODEL_PATH" \
    --text-encoder-path "$GEMMA_DIR"

# 5. è®­ç»ƒ
echo "ğŸ”¥ å¼€å§‹è®­ç»ƒ..."

# 5.1 åŠ¨æ€æ›´æ–°é…ç½®æ–‡ä»¶ (æ›¿æ¢å ä½ç¬¦ä¸ºçœŸå®è·¯å¾„)
CONFIG_FILE="configs/ltx2_av_lora.yaml"
PREPROCESSED_DIR="$(dirname "$DATASET_JSON")/.precomputed"

echo "æ­£åœ¨æ›´æ–°é…ç½®æ–‡ä»¶ $CONFIG_FILE..."
echo "  - Model Path: $LTX_MODEL_PATH"
echo "  - Text Encoder: $GEMMA_DIR"
echo "  - Data Root: $PREPROCESSED_DIR"

# ä½¿ç”¨ absolute path é˜²æ­¢è·¯å¾„é—®é¢˜ (å¯é€‰ï¼Œä½†æ¨è)
ABS_MODEL_PATH=$(readlink -f "$LTX_MODEL_PATH")
ABS_GEMMA_DIR=$(readlink -f "$GEMMA_DIR")
ABS_DATA_ROOT=$(readlink -f "$PREPROCESSED_DIR")

# ä½¿ç”¨ sed æ›¿æ¢ YAML ä¸­çš„å ä½ç¬¦
# æ³¨æ„ï¼šä½¿ç”¨ | ä½œä¸ºåˆ†éš”ç¬¦ï¼Œå› ä¸ºè·¯å¾„ä¸­åŒ…å« /
sed -i "s|model_path: \"path/to/ltx-2-model.safetensors\"|model_path: \"$ABS_MODEL_PATH\"|g" "$CONFIG_FILE"
sed -i "s|text_encoder_path: \"path/to/gemma-text-encoder\"|text_encoder_path: \"$ABS_GEMMA_DIR\"|g" "$CONFIG_FILE"
sed -i "s|preprocessed_data_root: \"/path/to/preprocessed/data\"|preprocessed_data_root: \"$ABS_DATA_ROOT\"|g" "$CONFIG_FILE"

# ç¦ç”¨éŸ³é¢‘è®­ç»ƒ (å› ä¸ºæ•°æ®é›†ä»…åŒ…å«è§†é¢‘/å­—å¹•ï¼Œä¸”é¢„å¤„ç†æœªç”ŸæˆéŸ³é¢‘æ½œå˜é‡)
echo "æ­£åœ¨è‡ªåŠ¨ç¦ç”¨éŸ³é¢‘è®­ç»ƒ (with_audio: false)..."
sed -i "s|with_audio: true|with_audio: false|g" "$CONFIG_FILE"

# é»˜è®¤ä½¿ç”¨ LoRA é…ç½®ï¼Œå¦‚æœéœ€è¦å…¨é‡å¾®è°ƒè¯·ä¿®æ”¹æ­¤å¤„çš„é…ç½®æ–‡ä»¶è·¯å¾„
uv run scripts/train.py "$CONFIG_FILE"

echo "âœ… è®­ç»ƒæµç¨‹å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ä½äº runs/ ç›®å½•ã€‚"
