
import os
import argparse
from pathlib import Path
from ltx_core.loader import LTXV_LORA_COMFY_RENAMING_MAP, LoraPathStrengthAndSDOps
from ltx_pipelines.ti2vid_two_stages import TI2VidTwoStagesPipeline

def run_inference(
    base_model_path: str,
    text_encoder_path: str,
    lora_path: str,
    output_path: str,
    prompt: str,
    negative_prompt: str,
    height: int = 512,
    width: int = 768,
    num_frames: int = 121,
    seed: int = 42,
    input_image: str = None
):
    """
    è¿è¡Œ LTX-2 æ¨¡å‹çš„æ¨ç†è„šæœ¬
    """
    
    # 1. è®¾ç½® LoRA é…ç½®
    # å¦‚æœæŒ‡å®šäº† LoRA è·¯å¾„ï¼Œåˆ™åŠ è½½å®ƒ
    loras = []
    if lora_path and os.path.exists(lora_path):
        print(f"ğŸ“¦ åŠ è½½ LoRA æ¨¡å‹: {lora_path}")
        loras.append(
            LoraPathStrengthAndSDOps(
                lora_path,
                1.0, # å¼ºåº¦ (0.0 - 1.0)
                LTXV_LORA_COMFY_RENAMING_MAP
            )
        )
    else:
        print("âš ï¸ æœªæŒ‡å®š LoRA è·¯å¾„æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä»…ä½¿ç”¨åŸºåº§æ¨¡å‹æ¨ç†ã€‚")

    # 2. åˆå§‹åŒ– Pipeline
    # æˆ‘ä»¬ä½¿ç”¨ä¸¤é˜¶æ®µ Pipeline ä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾ä¸éœ€è¦é¢å¤–çš„ upsampler æˆ– distilled loraï¼Œå¦‚æœéœ€è¦å¯ä»¥ä¿®æ”¹å‚æ•°
    # åœ¨ 19B æ¨¡å‹ä¸Šé€šå¸¸åªéœ€è¦åŸºæœ¬çš„é…ç½®
    print(f"ğŸš€ åˆå§‹åŒ– LTX-2 Pipeline...")
    print(f"   - åŸºåº§æ¨¡å‹: {base_model_path}")
    print(f"   - æ–‡æœ¬ç¼–ç å™¨: {text_encoder_path}")
    
    pipeline = TI2VidTwoStagesPipeline(
        checkpoint_path=base_model_path,
        distilled_lora=[], # æš‚ä¸ä½¿ç”¨ distilled lora
        spatial_upsampler_path=None, # å¦‚éœ€è¶…åˆ†å¯æ·»åŠ  upsampler è·¯å¾„
        gemma_root=text_encoder_path,
        loras=loras,
        fp8transformer=True # å¼€å¯ FP8 ä»¥èŠ‚çœæ˜¾å­˜ï¼Œæ˜¾å­˜è¶³å¤Ÿå¯è®¾ä¸º False
    )

    # æ„é€ å›¾ç‰‡è¾“å…¥å‚æ•°
    # æ ¼å¼: list[tuple[path, frame_idx, strength]]
    # æˆ‘ä»¬é»˜è®¤æ”¾åœ¨ç¬¬ 0 å¸§ï¼Œå¼ºåº¦ 1.0 (è¿™æ˜¯æœ€æ ‡å‡†çš„å›¾ç”Ÿè§†é¢‘ç”¨æ³•)
    images_arg = []
    if input_image:
        if not os.path.exists(input_image):
            print(f"âŒ é”™è¯¯ï¼šè¾“å…¥å›¾ç‰‡æœªæ‰¾åˆ°: {input_image}")
            return
        print(f"ğŸ–¼ï¸  ä½¿ç”¨å›¾ç‰‡ä½œä¸ºé¦–å¸§è¾“å…¥: {input_image}")
        images_arg = [(input_image, 0, 1.0)]

    # 3. ç”Ÿæˆè§†é¢‘
    print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘...")
    print(f"   - æç¤ºè¯: {prompt}")
    print(f"   - åˆ†è¾¨ç‡: {width}x{height}")
    print(f"   - å¸§æ•°: {num_frames}")

    output_file = pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        output_path=output_path,
        seed=seed,
        height=height,
        width=width,
        num_frames=num_frames,
        frame_rate=25.0,
        num_inference_steps=40, # æ¨ç†æ­¥æ•°ï¼Œè¶Šé«˜è¶Šç²¾ç»†ä½†è¶Šæ…¢
        cfg_guidance_scale=3.0, # æç¤ºè¯ç›¸å…³æ€§ï¼Œé€šå¸¸ 3.0-4.0
        images=images_arg  # <--- ä¼ å…¥å›¾ç‰‡å‚æ•°
    )
    
    print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆï¼å·²ä¿å­˜è‡³: {output_path}")

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="LTX-2 æ¨ç†è„šæœ¬")
    
    # æ¨¡å‹è·¯å¾„å‚æ•°
    parser.add_argument("--base-model", type=str, default="models/ltx-2-19b-dev.safetensors", help="LTX-2 åŸºåº§æ¨¡å‹è·¯å¾„")
    parser.add_argument("--text-encoder", type=str, default="models/gemma-text-encoder", help="Gemma æ–‡æœ¬ç¼–ç å™¨ç›®å½•")
    parser.add_argument("--lora", type=str, default="outputs/ltx2_av_lora/checkpoints/latest.safetensors", help="è®­ç»ƒå¥½çš„ LoRA æ–‡ä»¶è·¯å¾„")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--output", type=str, default="generated_video.mp4", help="è¾“å‡ºè§†é¢‘æ–‡ä»¶å")
    parser.add_argument("--prompt", type=str, required=True, help="è§†é¢‘ç”Ÿæˆçš„æç¤ºè¯ (è‹±æ–‡)")
    parser.add_argument("--negative-prompt", type=str, default="worst quality, blurry, jittery", help="è´Ÿé¢æç¤ºè¯")
    parser.add_argument("--input-image", type=str, default=None, help="[å¯é€‰] è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼Œç”¨äºå›¾ç”Ÿè§†é¢‘ (Image-to-Video)")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(args.base_model):
        print(f"âŒ é”™è¯¯ï¼šåŸºåº§æ¨¡å‹æœªæ‰¾åˆ°: {args.base_model}")
        exit(1)
        
    if not os.path.exists(args.text_encoder):
        print(f"âŒ é”™è¯¯ï¼šæ–‡æœ¬ç¼–ç å™¨æœªæ‰¾åˆ°: {args.text_encoder}")
        exit(1)

    run_inference(
        base_model_path=args.base_model,
        text_encoder_path=args.text_encoder,
        lora_path=args.lora,
        output_path=args.output,
        prompt=args.prompt,
        negative_prompt=args.negative_prompt,
        input_image=args.input_image
    )
